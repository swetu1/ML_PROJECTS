{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7779913,"sourceType":"datasetVersion","datasetId":4552572},{"sourceId":7803179,"sourceType":"datasetVersion","datasetId":4569338},{"sourceId":7803183,"sourceType":"datasetVersion","datasetId":4569341},{"sourceId":7786704,"sourceType":"datasetVersion","datasetId":4557576},{"sourceId":7806155,"sourceType":"datasetVersion","datasetId":4571442},{"sourceId":7806162,"sourceType":"datasetVersion","datasetId":4571447}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np  ","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:29:56.835558Z","iopub.execute_input":"2024-03-10T15:29:56.836249Z","iopub.status.idle":"2024-03-10T15:29:56.840585Z","shell.execute_reply.started":"2024-03-10T15:29:56.836206Z","shell.execute_reply":"2024-03-10T15:29:56.839583Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:29:58.849370Z","iopub.execute_input":"2024-03-10T15:29:58.850101Z","iopub.status.idle":"2024-03-10T15:29:58.854572Z","shell.execute_reply.started":"2024-03-10T15:29:58.850067Z","shell.execute_reply":"2024-03-10T15:29:58.853604Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv(\"/kaggle/input/piramal/dataset/train_main_loan.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:02.317289Z","iopub.execute_input":"2024-03-10T15:30:02.317655Z","iopub.status.idle":"2024-03-10T15:30:02.667424Z","shell.execute_reply.started":"2024-03-10T15:30:02.317629Z","shell.execute_reply":"2024-03-10T15:30:02.666620Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:05.020543Z","iopub.execute_input":"2024-03-10T15:30:05.020912Z","iopub.status.idle":"2024-03-10T15:30:05.124251Z","shell.execute_reply.started":"2024-03-10T15:30:05.020885Z","shell.execute_reply":"2024-03-10T15:30:05.123155Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 69958 entries, 0 to 69957\nData columns (total 24 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   ID                                69958 non-null  object \n 1   ACCOUNT_TYPE                      69958 non-null  object \n 2   HIGH_CREDIT_OR_SANCTIONED_AMOUNT  69958 non-null  int64  \n 3   DATE_OPENED                       69958 non-null  object \n 4   CURRENT_BALANCE                   69958 non-null  int64  \n 5   ACTUAL_PAYMT_AMT                  65741 non-null  float64\n 6   EMI_AMOUNT                        20192 non-null  float64\n 7   REPAYMENT_TENURE                  32782 non-null  float64\n 8   LOAN_CLASSIFICATION               69958 non-null  int64  \n 9   AMOUNT_OVERDUE                    9869 non-null   float64\n 10  PAYMENT_HISTORY_1                 69958 non-null  object \n 11  PAYMENT_HISTORY_2                 30285 non-null  object \n 12  OWNERSHIP_TYPE                    69958 non-null  object \n 13  COLLATERALVALUE                   46990 non-null  float64\n 14  TU_SCORE                          69958 non-null  int64  \n 15  PAYMENT_HISTORY_START_DATE        69958 non-null  object \n 16  PAYMENT_HISTORY_END_DATE          69958 non-null  object \n 17  DATE_REPORTED_AND_CERTIFIED       69958 non-null  object \n 18  DATE_OF_LAST_PAYMENT              66646 non-null  object \n 19  Reported_Date                     69958 non-null  object \n 20  DATE_OF_BIRTH                     69958 non-null  object \n 21  OCCUPATION_TYPE                   50845 non-null  object \n 22  GENDER                            69958 non-null  object \n 23  ACTUAL_ROI                        69958 non-null  float64\ndtypes: float64(6), int64(4), object(14)\nmemory usage: 12.8+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"mode_value = df['OCCUPATION_TYPE'].mode()[0]\ndf['OCCUPATION_TYPE'].fillna(value=mode_value, inplace=True)\ndf.loc[df['OCCUPATION_TYPE'] == \"R18\", 'OCCUPATION_TYPE'] = \"SALARIED\"\nunique_values = df['OCCUPATION_TYPE'].unique()\nunique_values","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:07.796167Z","iopub.execute_input":"2024-03-10T15:30:07.797034Z","iopub.status.idle":"2024-03-10T15:30:07.838792Z","shell.execute_reply.started":"2024-03-10T15:30:07.797002Z","shell.execute_reply":"2024-03-10T15:30:07.837931Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/486089511.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['OCCUPATION_TYPE'].fillna(value=mode_value, inplace=True)\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"array(['SALARIED', 'SENP', 'OTHERS', 'SEP'], dtype=object)"},"metadata":{}}]},{"cell_type":"code","source":"df = pd.get_dummies(df, columns=['GENDER', 'OWNERSHIP_TYPE',\"ACCOUNT_TYPE\",\"OCCUPATION_TYPE\"])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:10.746978Z","iopub.execute_input":"2024-03-10T15:30:10.747801Z","iopub.status.idle":"2024-03-10T15:30:10.804368Z","shell.execute_reply.started":"2024-03-10T15:30:10.747770Z","shell.execute_reply":"2024-03-10T15:30:10.803447Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"df= df.astype(int, errors='ignore')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:12.855038Z","iopub.execute_input":"2024-03-10T15:30:12.855929Z","iopub.status.idle":"2024-03-10T15:30:12.863582Z","shell.execute_reply.started":"2024-03-10T15:30:12.855886Z","shell.execute_reply":"2024-03-10T15:30:12.862481Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'], format='%d-%m-%Y')\ndf['DATE_OPENED'] = pd.to_datetime(df['DATE_OPENED'], format='%d-%m-%Y')\ndf['age of person'] = (df['DATE_OPENED'] - df['DATE_OF_BIRTH']).dt.days / 365.25","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:15.831626Z","iopub.execute_input":"2024-03-10T15:30:15.832566Z","iopub.status.idle":"2024-03-10T15:30:16.389928Z","shell.execute_reply.started":"2024-03-10T15:30:15.832534Z","shell.execute_reply":"2024-03-10T15:30:16.388939Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"df['AMOUNT_OVERDUE'].fillna(0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:20.398167Z","iopub.execute_input":"2024-03-10T15:30:20.398816Z","iopub.status.idle":"2024-03-10T15:30:20.405217Z","shell.execute_reply.started":"2024-03-10T15:30:20.398787Z","shell.execute_reply":"2024-03-10T15:30:20.404068Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/1861289169.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  df['AMOUNT_OVERDUE'].fillna(0, inplace=True)\n","output_type":"stream"}]},{"cell_type":"code","source":"df[\"total amount\"]=df[\"HIGH_CREDIT_OR_SANCTIONED_AMOUNT\"]-df[\"CURRENT_BALANCE\"]+df[\"AMOUNT_OVERDUE\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:30:24.001756Z","iopub.execute_input":"2024-03-10T15:30:24.002386Z","iopub.status.idle":"2024-03-10T15:30:24.008363Z","shell.execute_reply.started":"2024-03-10T15:30:24.002357Z","shell.execute_reply":"2024-03-10T15:30:24.007486Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"df['Reported_Date'] = pd.to_datetime(df['Reported_Date'], format='%d-%m-%Y')\ndf[\"date_opened_reported_date\"]=(df['DATE_OPENED'] - df['Reported_Date']).dt.days / 365.25","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:06.287759Z","iopub.execute_input":"2024-03-10T15:31:06.288171Z","iopub.status.idle":"2024-03-10T15:31:06.310415Z","shell.execute_reply.started":"2024-03-10T15:31:06.288136Z","shell.execute_reply":"2024-03-10T15:31:06.309474Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['DATE_OPENED','PAYMENT_HISTORY_START_DATE','Reported_Date','DATE_OF_BIRTH',\"DATE_OF_LAST_PAYMENT\",\"DATE_REPORTED_AND_CERTIFIED\",\"PAYMENT_HISTORY_END_DATE\"], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:09.211891Z","iopub.execute_input":"2024-03-10T15:31:09.212635Z","iopub.status.idle":"2024-03-10T15:31:09.238644Z","shell.execute_reply.started":"2024-03-10T15:31:09.212606Z","shell.execute_reply":"2024-03-10T15:31:09.237914Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:11.770815Z","iopub.execute_input":"2024-03-10T15:31:11.771617Z","iopub.status.idle":"2024-03-10T15:31:11.803941Z","shell.execute_reply.started":"2024-03-10T15:31:11.771588Z","shell.execute_reply":"2024-03-10T15:31:11.803105Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 69958 entries, 0 to 69957\nData columns (total 29 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   ID                                69958 non-null  object \n 1   HIGH_CREDIT_OR_SANCTIONED_AMOUNT  69958 non-null  int64  \n 2   CURRENT_BALANCE                   69958 non-null  int64  \n 3   ACTUAL_PAYMT_AMT                  65741 non-null  float64\n 4   EMI_AMOUNT                        20192 non-null  float64\n 5   REPAYMENT_TENURE                  32782 non-null  float64\n 6   LOAN_CLASSIFICATION               69958 non-null  int64  \n 7   AMOUNT_OVERDUE                    69958 non-null  float64\n 8   PAYMENT_HISTORY_1                 69958 non-null  object \n 9   PAYMENT_HISTORY_2                 30285 non-null  object \n 10  COLLATERALVALUE                   46990 non-null  float64\n 11  TU_SCORE                          69958 non-null  int64  \n 12  ACTUAL_ROI                        69958 non-null  float64\n 13  GENDER_Female                     69958 non-null  int64  \n 14  GENDER_Male                       69958 non-null  int64  \n 15  GENDER_Other                      69958 non-null  int64  \n 16  OWNERSHIP_TYPE_Individual         69958 non-null  int64  \n 17  OWNERSHIP_TYPE_Joint              69958 non-null  int64  \n 18  ACCOUNT_TYPE_Business Loan        69958 non-null  int64  \n 19  ACCOUNT_TYPE_Housing Loan         69958 non-null  int64  \n 20  ACCOUNT_TYPE_Personal Loan        69958 non-null  int64  \n 21  ACCOUNT_TYPE_Property Loan        69958 non-null  int64  \n 22  OCCUPATION_TYPE_OTHERS            69958 non-null  int64  \n 23  OCCUPATION_TYPE_SALARIED          69958 non-null  int64  \n 24  OCCUPATION_TYPE_SENP              69958 non-null  int64  \n 25  OCCUPATION_TYPE_SEP               69958 non-null  int64  \n 26  age of person                     69958 non-null  float64\n 27  total amount                      69958 non-null  float64\n 28  date_opened_reported_date         69958 non-null  float64\ndtypes: float64(9), int64(17), object(3)\nmemory usage: 15.5+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# df.drop(columns=['REPAYMENT_TENURE'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:26.787199Z","iopub.execute_input":"2024-03-10T15:31:26.787914Z","iopub.status.idle":"2024-03-10T15:31:26.791737Z","shell.execute_reply.started":"2024-03-10T15:31:26.787885Z","shell.execute_reply":"2024-03-10T15:31:26.790711Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"history1=pd.read_csv(\"/kaggle/input/dff-new/PAYMENT_HISTORY_1.csv\")\nhistory2=pd.read_csv(\"/kaggle/input/dff-new/PAYMENT_HISTORY_2.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:28.805697Z","iopub.execute_input":"2024-03-10T15:31:28.806595Z","iopub.status.idle":"2024-03-10T15:31:28.902602Z","shell.execute_reply.started":"2024-03-10T15:31:28.806563Z","shell.execute_reply":"2024-03-10T15:31:28.901681Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"def process_payment_history(entry):\n    # Check if the entry is not a string (including NaN/None which are float in pandas)\n    if not isinstance(entry, str):\n        return float('nan'), 0, 0, 0, 0, 0, 0\n    # Replacement rules\n    replacements = {'STD': '000', 'SMA': '060', 'SUB': '090', 'DBT': '090', 'LSS': '090', 'XXX': 'XXX'}\n    \n    # Replace according to rules\n    for key, value in replacements.items():\n        entry = entry.replace(key, value)\n    # Initialize counts and total for average calculation\n    count_xxx = entry.count('XXX')\n    nums = []  \n    i = 0\n    zero=0\n    zero_30=0\n    thirty_60=0\n    sixty_90=0\n    ninety_plus=0\n    while i < len(entry):\n        group = entry[i:i+3]  \n\n        if 'X' in group:\n            number_before_x = ''.join([char for char in group if char.isdigit()])\n            if number_before_x:  \n                nums.append(int(number_before_x))\n\n            i += group.find('X')  \n            while i < len(entry) and entry[i] == 'X':\n                i += 1  # Skip the 'X's\n        else:\n            try:\n                if group!=\"060\" and group!=\"090\":\n                    nums.append(int(group))\n            except ValueError:\n                # This catches cases where conversion to integer might fail\n                pass\n            i += 3  # Move to the next group\n    \n    for i in range(0,len(nums)):\n        num=nums[i]\n        if num == 0:\n            zero += 1\n        elif 0 < num <= 30:\n            zero_30 += 1\n        elif 30 < num <= 60:\n            # Check if the entry is not a string (including NaN/None which are float in pandas)\n            thirty_60 += 1\n        elif 60 < num <= 90:\n            sixty_90 += 1\n        elif num > 90:\n            ninety_plus += 1\n    avg_nums = sum(nums)\n    if len(nums)==0:\n        t=0\n    else:\n        t= nums[len(nums)-1]\n    if len(nums)==0:\n        t1=0\n    else:\n        t1= nums[0]\n    return avg_nums, count_xxx,zero, zero_30 , thirty_60 , sixty_90,ninety_plus, t,t1\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:31.016457Z","iopub.execute_input":"2024-03-10T15:31:31.016842Z","iopub.status.idle":"2024-03-10T15:31:31.030067Z","shell.execute_reply.started":"2024-03-10T15:31:31.016812Z","shell.execute_reply":"2024-03-10T15:31:31.029210Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Create a mask for rows in df where PAYMENT_HISTORY_1 contains 'E'\nmask = df['PAYMENT_HISTORY_1'].str.contains('E')\n\n# Create a mapping series from history_1\nmapping_series = history1.set_index('ID')['PAYMENT_HISTORY_1']\n\n# Replace values in df where mask is True by mapping using the 'ID' column\ndf.loc[mask, 'PAYMENT_HISTORY_1'] = df.loc[mask, 'ID'].map(mapping_series)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:40.916041Z","iopub.execute_input":"2024-03-10T15:31:40.916432Z","iopub.status.idle":"2024-03-10T15:31:40.959882Z","shell.execute_reply.started":"2024-03-10T15:31:40.916402Z","shell.execute_reply":"2024-03-10T15:31:40.959010Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Create a mask for rows in df where PAYMENT_HISTORY_1 contains 'E'\nmask = df['PAYMENT_HISTORY_2'].notnull() & df['PAYMENT_HISTORY_2'].str.contains('E')\n\n# Create a mapping series from history_1\nmapping_series = history2.set_index('ID')['PAYMENT_HISTORY_2']\n\n# Replace values in df where mask is True by mapping using the 'ID' column\ndf.loc[mask, 'PAYMENT_HISTORY_2'] = df.loc[mask, 'ID'].map(mapping_series)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:43.461098Z","iopub.execute_input":"2024-03-10T15:31:43.462002Z","iopub.status.idle":"2024-03-10T15:31:43.511655Z","shell.execute_reply.started":"2024-03-10T15:31:43.461970Z","shell.execute_reply":"2024-03-10T15:31:43.510931Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"def convert_sci_notation(s):\n    try:\n        # Convert to float, then to integer, and finally to string to exclude the decimal part\n        return '{:.0f}'.format(float(s))\n    except ValueError:\n        # Return the original string if it cannot be converted to float\n        return s\ndf['PAYMENT_HISTORY_1'] = df['PAYMENT_HISTORY_1'].apply(convert_sci_notation)\ndf['PAYMENT_HISTORY_2'] = df['PAYMENT_HISTORY_2'].apply(convert_sci_notation)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:46.607190Z","iopub.execute_input":"2024-03-10T15:31:46.608280Z","iopub.status.idle":"2024-03-10T15:31:46.743994Z","shell.execute_reply.started":"2024-03-10T15:31:46.608252Z","shell.execute_reply":"2024-03-10T15:31:46.742917Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the PAYMENT_HISTORY_1 column\nresults1 = df['PAYMENT_HISTORY_1'].apply(process_payment_history)\n\n# Split the results into separate columns\ndf['avg_nums1'], df['count_xxx1'], df['zero1'], df['zero_301'], df['thirty_601'], df['sixty_901'], df['ninety_plus1'], df['firstlaste1'] ,df['lastlate1']= zip(*results1)\n\n# Apply the function to the PAYMENT_HISTORY_2 column\nresults2 = df['PAYMENT_HISTORY_2'].apply(process_payment_history)\n\n# Split the results into separate columns\ndf['avg_nums2'], df['count_xxx2'], df['zero2'], df['zero_302'], df['thirty_602'], df['sixty_902'], df['ninety_plus2'], df['firstlaste2'] ,df['lastlate2'] = zip(*results2)\n\n# Calculate combined averages and counts\ndf['avg_nums_combined'] = (df['avg_nums1'] + df['avg_nums2']) / 36\ndf['count_xxx_combined'] = df['count_xxx1'] + df['count_xxx2']\ndf['zero_combined'] = df['zero1'] + df['zero2']\ndf['zero_30_combined'] = df['zero_301'] + df['zero_302']\ndf['thirty_60_combined'] = df['thirty_601'] + df['thirty_602']\ndf['sixty_90_combined'] = df['sixty_901'] + df['sixty_902']\ndf['ninety_plus_combined'] = df['ninety_plus1'] + df['ninety_plus2']\ndf['firstlaste1 combined']=df['firstlaste2']\ndf['lastlate1']=df['lastlate1']\n# Drop the intermediate columns\ndf = df.drop(columns=['avg_nums1', 'count_xxx1', 'zero1', 'zero_301', 'thirty_601', 'sixty_901', 'ninety_plus1',\n                      'avg_nums2', 'count_xxx2', 'zero2', 'zero_302', 'thirty_602', 'sixty_902', 'ninety_plus2',\n                      'PAYMENT_HISTORY_1', 'PAYMENT_HISTORY_2','firstlaste1','firstlaste2','lastlate1','lastlate2'])\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:48.725492Z","iopub.execute_input":"2024-03-10T15:31:48.726485Z","iopub.status.idle":"2024-03-10T15:31:50.901381Z","shell.execute_reply.started":"2024-03-10T15:31:48.726452Z","shell.execute_reply":"2024-03-10T15:31:50.900617Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:31:54.306123Z","iopub.execute_input":"2024-03-10T15:31:54.306952Z","iopub.status.idle":"2024-03-10T15:31:54.330001Z","shell.execute_reply.started":"2024-03-10T15:31:54.306923Z","shell.execute_reply":"2024-03-10T15:31:54.329124Z"},"trusted":true},"execution_count":36,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 69958 entries, 0 to 69957\nData columns (total 35 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   ID                                69958 non-null  object \n 1   HIGH_CREDIT_OR_SANCTIONED_AMOUNT  69958 non-null  int64  \n 2   CURRENT_BALANCE                   69958 non-null  int64  \n 3   ACTUAL_PAYMT_AMT                  65741 non-null  float64\n 4   EMI_AMOUNT                        20192 non-null  float64\n 5   REPAYMENT_TENURE                  32782 non-null  float64\n 6   LOAN_CLASSIFICATION               69958 non-null  int64  \n 7   AMOUNT_OVERDUE                    69958 non-null  float64\n 8   COLLATERALVALUE                   46990 non-null  float64\n 9   TU_SCORE                          69958 non-null  int64  \n 10  ACTUAL_ROI                        69958 non-null  float64\n 11  GENDER_Female                     69958 non-null  int64  \n 12  GENDER_Male                       69958 non-null  int64  \n 13  GENDER_Other                      69958 non-null  int64  \n 14  OWNERSHIP_TYPE_Individual         69958 non-null  int64  \n 15  OWNERSHIP_TYPE_Joint              69958 non-null  int64  \n 16  ACCOUNT_TYPE_Business Loan        69958 non-null  int64  \n 17  ACCOUNT_TYPE_Housing Loan         69958 non-null  int64  \n 18  ACCOUNT_TYPE_Personal Loan        69958 non-null  int64  \n 19  ACCOUNT_TYPE_Property Loan        69958 non-null  int64  \n 20  OCCUPATION_TYPE_OTHERS            69958 non-null  int64  \n 21  OCCUPATION_TYPE_SALARIED          69958 non-null  int64  \n 22  OCCUPATION_TYPE_SENP              69958 non-null  int64  \n 23  OCCUPATION_TYPE_SEP               69958 non-null  int64  \n 24  age of person                     69958 non-null  float64\n 25  total amount                      69958 non-null  float64\n 26  date_opened_reported_date         69958 non-null  float64\n 27  avg_nums_combined                 69958 non-null  float64\n 28  count_xxx_combined                69958 non-null  int64  \n 29  zero_combined                     69958 non-null  int64  \n 30  zero_30_combined                  69958 non-null  int64  \n 31  thirty_60_combined                69958 non-null  int64  \n 32  sixty_90_combined                 69958 non-null  int64  \n 33  ninety_plus_combined              69958 non-null  int64  \n 34  firstlaste1 combined              69958 non-null  int64  \ndtypes: float64(10), int64(24), object(1)\nmemory usage: 18.7+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"df1=df.drop(columns=['ACTUAL_ROI'])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:28.997977Z","iopub.execute_input":"2024-03-10T15:32:28.998390Z","iopub.status.idle":"2024-03-10T15:32:29.011813Z","shell.execute_reply.started":"2024-03-10T15:32:28.998360Z","shell.execute_reply":"2024-03-10T15:32:29.010596Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\n\n\n# Create a MinMaxScaler object\nscaler = MinMaxScaler()\n\n# Select only the numeric columns for normalization\nnumeric_cols = df1.select_dtypes(include=['int64', 'float64']).columns\n\n# Apply the scaler to the DataFrame\ndf1[numeric_cols] = scaler.fit_transform(df1[numeric_cols])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:31.614268Z","iopub.execute_input":"2024-03-10T15:32:31.615059Z","iopub.status.idle":"2024-03-10T15:32:32.305707Z","shell.execute_reply.started":"2024-03-10T15:32:31.615031Z","shell.execute_reply":"2024-03-10T15:32:32.304902Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"df1[:5]","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:33.961251Z","iopub.execute_input":"2024-03-10T15:32:33.961772Z","iopub.status.idle":"2024-03-10T15:32:34.006370Z","shell.execute_reply.started":"2024-03-10T15:32:33.961730Z","shell.execute_reply":"2024-03-10T15:32:34.005542Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"           ID  HIGH_CREDIT_OR_SANCTIONED_AMOUNT  CURRENT_BALANCE  \\\n0  A002338349                          0.001508         0.001236   \n1  A002000537                          0.002295         0.001832   \n2  A002421579                          0.003374         0.002504   \n3  A002152345                          0.003414         0.003329   \n4  A001952834                          0.004285         0.003458   \n\n   ACTUAL_PAYMT_AMT  EMI_AMOUNT  REPAYMENT_TENURE  LOAN_CLASSIFICATION  \\\n0          0.000640    0.002328          0.181818             0.000000   \n1          0.000037    0.003296          0.201102             0.000000   \n2          0.001189    0.004330          0.190083             0.000000   \n3          0.002002         NaN               NaN             0.000000   \n4          0.001864         NaN          0.146006             0.598889   \n\n   AMOUNT_OVERDUE  COLLATERALVALUE  TU_SCORE  GENDER_Female  GENDER_Male  \\\n0         0.00000         0.002873  0.465409            0.0          1.0   \n1         0.00000         0.002115  0.789308            1.0          0.0   \n2         0.00000         0.004941  0.676101            0.0          1.0   \n3         0.00000         0.004200  0.795597            1.0          0.0   \n4         0.00139         0.003836  0.411950            0.0          1.0   \n\n   GENDER_Other  OWNERSHIP_TYPE_Individual  OWNERSHIP_TYPE_Joint  \\\n0           0.0                        0.0                   1.0   \n1           0.0                        0.0                   1.0   \n2           0.0                        0.0                   1.0   \n3           0.0                        0.0                   1.0   \n4           0.0                        0.0                   1.0   \n\n   ACCOUNT_TYPE_Business Loan  ACCOUNT_TYPE_Housing Loan  \\\n0                         0.0                        1.0   \n1                         0.0                        1.0   \n2                         0.0                        1.0   \n3                         0.0                        1.0   \n4                         0.0                        1.0   \n\n   ACCOUNT_TYPE_Personal Loan  ACCOUNT_TYPE_Property Loan  \\\n0                         0.0                         0.0   \n1                         0.0                         0.0   \n2                         0.0                         0.0   \n3                         0.0                         0.0   \n4                         0.0                         0.0   \n\n   OCCUPATION_TYPE_OTHERS  OCCUPATION_TYPE_SALARIED  OCCUPATION_TYPE_SENP  \\\n0                     0.0                       1.0                   0.0   \n1                     0.0                       1.0                   0.0   \n2                     0.0                       1.0                   0.0   \n3                     0.0                       1.0                   0.0   \n4                     0.0                       1.0                   0.0   \n\n   OCCUPATION_TYPE_SEP  age of person  total amount  \\\n0                  0.0       0.340078      0.012418   \n1                  0.0       0.136284      0.012764   \n2                  0.0       0.171062      0.013463   \n3                  0.0       0.480826      0.012258   \n4                  0.0       0.298196      0.014737   \n\n   date_opened_reported_date  avg_nums_combined  count_xxx_combined  \\\n0                   0.560234           0.028272            0.035714   \n1                   0.489632           0.000000            0.035714   \n2                   0.688335           0.000000            0.035714   \n3                   0.956621           0.000000            0.000000   \n4                   0.576771           0.622458            0.035714   \n\n   zero_combined  zero_30_combined  thirty_60_combined  sixty_90_combined  \\\n0       0.529412          0.043478            0.166667           0.000000   \n1       0.529412          0.000000            0.000000           0.000000   \n2       0.529412          0.000000            0.000000           0.000000   \n3       0.029412          0.000000            0.000000           0.000000   \n4       0.000000          0.000000            0.055556           0.058824   \n\n   ninety_plus_combined  firstlaste1 combined  \n0              0.057143              0.000000  \n1              0.000000              0.000000  \n2              0.000000              0.000000  \n3              0.000000              0.000000  \n4              0.942857              0.596774  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ID</th>\n      <th>HIGH_CREDIT_OR_SANCTIONED_AMOUNT</th>\n      <th>CURRENT_BALANCE</th>\n      <th>ACTUAL_PAYMT_AMT</th>\n      <th>EMI_AMOUNT</th>\n      <th>REPAYMENT_TENURE</th>\n      <th>LOAN_CLASSIFICATION</th>\n      <th>AMOUNT_OVERDUE</th>\n      <th>COLLATERALVALUE</th>\n      <th>TU_SCORE</th>\n      <th>GENDER_Female</th>\n      <th>GENDER_Male</th>\n      <th>GENDER_Other</th>\n      <th>OWNERSHIP_TYPE_Individual</th>\n      <th>OWNERSHIP_TYPE_Joint</th>\n      <th>ACCOUNT_TYPE_Business Loan</th>\n      <th>ACCOUNT_TYPE_Housing Loan</th>\n      <th>ACCOUNT_TYPE_Personal Loan</th>\n      <th>ACCOUNT_TYPE_Property Loan</th>\n      <th>OCCUPATION_TYPE_OTHERS</th>\n      <th>OCCUPATION_TYPE_SALARIED</th>\n      <th>OCCUPATION_TYPE_SENP</th>\n      <th>OCCUPATION_TYPE_SEP</th>\n      <th>age of person</th>\n      <th>total amount</th>\n      <th>date_opened_reported_date</th>\n      <th>avg_nums_combined</th>\n      <th>count_xxx_combined</th>\n      <th>zero_combined</th>\n      <th>zero_30_combined</th>\n      <th>thirty_60_combined</th>\n      <th>sixty_90_combined</th>\n      <th>ninety_plus_combined</th>\n      <th>firstlaste1 combined</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>A002338349</td>\n      <td>0.001508</td>\n      <td>0.001236</td>\n      <td>0.000640</td>\n      <td>0.002328</td>\n      <td>0.181818</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.002873</td>\n      <td>0.465409</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.340078</td>\n      <td>0.012418</td>\n      <td>0.560234</td>\n      <td>0.028272</td>\n      <td>0.035714</td>\n      <td>0.529412</td>\n      <td>0.043478</td>\n      <td>0.166667</td>\n      <td>0.000000</td>\n      <td>0.057143</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>A002000537</td>\n      <td>0.002295</td>\n      <td>0.001832</td>\n      <td>0.000037</td>\n      <td>0.003296</td>\n      <td>0.201102</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.002115</td>\n      <td>0.789308</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.136284</td>\n      <td>0.012764</td>\n      <td>0.489632</td>\n      <td>0.000000</td>\n      <td>0.035714</td>\n      <td>0.529412</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>A002421579</td>\n      <td>0.003374</td>\n      <td>0.002504</td>\n      <td>0.001189</td>\n      <td>0.004330</td>\n      <td>0.190083</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.004941</td>\n      <td>0.676101</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.171062</td>\n      <td>0.013463</td>\n      <td>0.688335</td>\n      <td>0.000000</td>\n      <td>0.035714</td>\n      <td>0.529412</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>A002152345</td>\n      <td>0.003414</td>\n      <td>0.003329</td>\n      <td>0.002002</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.004200</td>\n      <td>0.795597</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.480826</td>\n      <td>0.012258</td>\n      <td>0.956621</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.029412</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A001952834</td>\n      <td>0.004285</td>\n      <td>0.003458</td>\n      <td>0.001864</td>\n      <td>NaN</td>\n      <td>0.146006</td>\n      <td>0.598889</td>\n      <td>0.00139</td>\n      <td>0.003836</td>\n      <td>0.411950</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.298196</td>\n      <td>0.014737</td>\n      <td>0.576771</td>\n      <td>0.622458</td>\n      <td>0.035714</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.055556</td>\n      <td>0.058824</td>\n      <td>0.942857</td>\n      <td>0.596774</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# import numpy as np\n# from sklearn.impute import KNNImputer\n# imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")\n\n# # Applying imputer on selected features\n# # Assuming 'df' is your original DataFrame and it includes all the necessary columns\n# selected_columns = [ 'ACTUAL_PAYMT_AMT', 'EMI_AMOUNT', 'COLLATERALVALUE','REPAYMENT_TENURE','HIGH_CREDIT_OR_SANCTIONED_AMOUNT','CURRENT_BALANCE','OWNERSHIP_TYPE_Individual','OWNERSHIP_TYPE_Joint','ACCOUNT_TYPE_Personal Loan']\n\n# imputed_data = imputer.fit_transform(df[selected_columns])\n\n# # Convert the imputed data back to a DataFrame (optional step to replace only the selected columns in original DataFrame)\n# imputed_df = pd.DataFrame(imputed_data, columns=selected_columns)\n\n# # If you want to replace the original columns in df with the imputed values\n# df[selected_columns] = imputed_df","metadata":{"execution":{"iopub.status.busy":"2024-03-10T10:03:18.619283Z","iopub.execute_input":"2024-03-10T10:03:18.619651Z","iopub.status.idle":"2024-03-10T10:03:18.624912Z","shell.execute_reply.started":"2024-03-10T10:03:18.619624Z","shell.execute_reply":"2024-03-10T10:03:18.623893Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"df_miss = df.drop(columns=['ACTUAL_ROI'])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:41.134665Z","iopub.execute_input":"2024-03-10T15:32:41.135567Z","iopub.status.idle":"2024-03-10T15:32:41.151120Z","shell.execute_reply.started":"2024-03-10T15:32:41.135535Z","shell.execute_reply":"2024-03-10T15:32:41.150282Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:46.088228Z","iopub.execute_input":"2024-03-10T15:32:46.088591Z","iopub.status.idle":"2024-03-10T15:32:46.108813Z","shell.execute_reply.started":"2024-03-10T15:32:46.088563Z","shell.execute_reply":"2024-03-10T15:32:46.107831Z"},"trusted":true},"execution_count":41,"outputs":[{"execution_count":41,"output_type":"execute_result","data":{"text/plain":"ID                                      0\nHIGH_CREDIT_OR_SANCTIONED_AMOUNT        0\nCURRENT_BALANCE                         0\nACTUAL_PAYMT_AMT                     4217\nEMI_AMOUNT                          49766\nREPAYMENT_TENURE                    37176\nLOAN_CLASSIFICATION                     0\nAMOUNT_OVERDUE                          0\nCOLLATERALVALUE                     22968\nTU_SCORE                                0\nGENDER_Female                           0\nGENDER_Male                             0\nGENDER_Other                            0\nOWNERSHIP_TYPE_Individual               0\nOWNERSHIP_TYPE_Joint                    0\nACCOUNT_TYPE_Business Loan              0\nACCOUNT_TYPE_Housing Loan               0\nACCOUNT_TYPE_Personal Loan              0\nACCOUNT_TYPE_Property Loan              0\nOCCUPATION_TYPE_OTHERS                  0\nOCCUPATION_TYPE_SALARIED                0\nOCCUPATION_TYPE_SENP                    0\nOCCUPATION_TYPE_SEP                     0\nage of person                           0\ntotal amount                            0\ndate_opened_reported_date               0\navg_nums_combined                       0\ncount_xxx_combined                      0\nzero_combined                           0\nzero_30_combined                        0\nthirty_60_combined                      0\nsixty_90_combined                       0\nninety_plus_combined                    0\nfirstlaste1 combined                    0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:48.970682Z","iopub.execute_input":"2024-03-10T15:32:48.971447Z","iopub.status.idle":"2024-03-10T15:32:49.391971Z","shell.execute_reply.started":"2024-03-10T15:32:48.971415Z","shell.execute_reply":"2024-03-10T15:32:49.390995Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"imputer = IterativeImputer(max_iter=10, random_state=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:52.686240Z","iopub.execute_input":"2024-03-10T15:32:52.686572Z","iopub.status.idle":"2024-03-10T15:32:52.691130Z","shell.execute_reply.started":"2024-03-10T15:32:52.686548Z","shell.execute_reply":"2024-03-10T15:32:52.689970Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"selected_columns = [ 'ACTUAL_PAYMT_AMT', 'EMI_AMOUNT', 'COLLATERALVALUE','REPAYMENT_TENURE','HIGH_CREDIT_OR_SANCTIONED_AMOUNT','CURRENT_BALANCE','OWNERSHIP_TYPE_Individual','OWNERSHIP_TYPE_Joint','ACCOUNT_TYPE_Personal Loan']","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:55.829957Z","iopub.execute_input":"2024-03-10T15:32:55.830317Z","iopub.status.idle":"2024-03-10T15:32:55.835024Z","shell.execute_reply.started":"2024-03-10T15:32:55.830290Z","shell.execute_reply":"2024-03-10T15:32:55.834140Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"imputed_data = imputer.fit_transform(df1[selected_columns])","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:32:58.674399Z","iopub.execute_input":"2024-03-10T15:32:58.675204Z","iopub.status.idle":"2024-03-10T15:33:04.038518Z","shell.execute_reply.started":"2024-03-10T15:32:58.675173Z","shell.execute_reply":"2024-03-10T15:33:04.037324Z"},"trusted":true},"execution_count":45,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/impute/_iterative.py:785: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"imputed_df = pd.DataFrame(imputed_data, columns=selected_columns)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:33:07.240858Z","iopub.execute_input":"2024-03-10T15:33:07.241555Z","iopub.status.idle":"2024-03-10T15:33:07.246114Z","shell.execute_reply.started":"2024-03-10T15:33:07.241523Z","shell.execute_reply":"2024-03-10T15:33:07.245232Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"df1[selected_columns] = imputed_df","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:33:13.032357Z","iopub.execute_input":"2024-03-10T15:33:13.032745Z","iopub.status.idle":"2024-03-10T15:33:13.040246Z","shell.execute_reply.started":"2024-03-10T15:33:13.032713Z","shell.execute_reply":"2024-03-10T15:33:13.039169Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"df1.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:33:16.364105Z","iopub.execute_input":"2024-03-10T15:33:16.364462Z","iopub.status.idle":"2024-03-10T15:33:16.386552Z","shell.execute_reply.started":"2024-03-10T15:33:16.364436Z","shell.execute_reply":"2024-03-10T15:33:16.385525Z"},"trusted":true},"execution_count":48,"outputs":[{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"ID                                  0\nHIGH_CREDIT_OR_SANCTIONED_AMOUNT    0\nCURRENT_BALANCE                     0\nACTUAL_PAYMT_AMT                    0\nEMI_AMOUNT                          0\nREPAYMENT_TENURE                    0\nLOAN_CLASSIFICATION                 0\nAMOUNT_OVERDUE                      0\nCOLLATERALVALUE                     0\nTU_SCORE                            0\nGENDER_Female                       0\nGENDER_Male                         0\nGENDER_Other                        0\nOWNERSHIP_TYPE_Individual           0\nOWNERSHIP_TYPE_Joint                0\nACCOUNT_TYPE_Business Loan          0\nACCOUNT_TYPE_Housing Loan           0\nACCOUNT_TYPE_Personal Loan          0\nACCOUNT_TYPE_Property Loan          0\nOCCUPATION_TYPE_OTHERS              0\nOCCUPATION_TYPE_SALARIED            0\nOCCUPATION_TYPE_SENP                0\nOCCUPATION_TYPE_SEP                 0\nage of person                       0\ntotal amount                        0\ndate_opened_reported_date           0\navg_nums_combined                   0\ncount_xxx_combined                  0\nzero_combined                       0\nzero_30_combined                    0\nthirty_60_combined                  0\nsixty_90_combined                   0\nninety_plus_combined                0\nfirstlaste1 combined                0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"df1.info()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:33:20.164265Z","iopub.execute_input":"2024-03-10T15:33:20.165303Z","iopub.status.idle":"2024-03-10T15:33:20.202305Z","shell.execute_reply.started":"2024-03-10T15:33:20.165262Z","shell.execute_reply":"2024-03-10T15:33:20.201348Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 69958 entries, 0 to 69957\nData columns (total 34 columns):\n #   Column                            Non-Null Count  Dtype  \n---  ------                            --------------  -----  \n 0   ID                                69958 non-null  object \n 1   HIGH_CREDIT_OR_SANCTIONED_AMOUNT  69958 non-null  float64\n 2   CURRENT_BALANCE                   69958 non-null  float64\n 3   ACTUAL_PAYMT_AMT                  69958 non-null  float64\n 4   EMI_AMOUNT                        69958 non-null  float64\n 5   REPAYMENT_TENURE                  69958 non-null  float64\n 6   LOAN_CLASSIFICATION               69958 non-null  float64\n 7   AMOUNT_OVERDUE                    69958 non-null  float64\n 8   COLLATERALVALUE                   69958 non-null  float64\n 9   TU_SCORE                          69958 non-null  float64\n 10  GENDER_Female                     69958 non-null  float64\n 11  GENDER_Male                       69958 non-null  float64\n 12  GENDER_Other                      69958 non-null  float64\n 13  OWNERSHIP_TYPE_Individual         69958 non-null  float64\n 14  OWNERSHIP_TYPE_Joint              69958 non-null  float64\n 15  ACCOUNT_TYPE_Business Loan        69958 non-null  float64\n 16  ACCOUNT_TYPE_Housing Loan         69958 non-null  float64\n 17  ACCOUNT_TYPE_Personal Loan        69958 non-null  float64\n 18  ACCOUNT_TYPE_Property Loan        69958 non-null  float64\n 19  OCCUPATION_TYPE_OTHERS            69958 non-null  float64\n 20  OCCUPATION_TYPE_SALARIED          69958 non-null  float64\n 21  OCCUPATION_TYPE_SENP              69958 non-null  float64\n 22  OCCUPATION_TYPE_SEP               69958 non-null  float64\n 23  age of person                     69958 non-null  float64\n 24  total amount                      69958 non-null  float64\n 25  date_opened_reported_date         69958 non-null  float64\n 26  avg_nums_combined                 69958 non-null  float64\n 27  count_xxx_combined                69958 non-null  float64\n 28  zero_combined                     69958 non-null  float64\n 29  zero_30_combined                  69958 non-null  float64\n 30  thirty_60_combined                69958 non-null  float64\n 31  sixty_90_combined                 69958 non-null  float64\n 32  ninety_plus_combined              69958 non-null  float64\n 33  firstlaste1 combined              69958 non-null  float64\ndtypes: float64(33), object(1)\nmemory usage: 18.1+ MB\n","output_type":"stream"}]},{"cell_type":"code","source":"# spearman_corr = df.corr(method='spearman')\n# kendall_tau = df.corr(method='kendall')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:39.193322Z","iopub.execute_input":"2024-03-10T15:34:39.194051Z","iopub.status.idle":"2024-03-10T15:34:39.197733Z","shell.execute_reply.started":"2024-03-10T15:34:39.194019Z","shell.execute_reply":"2024-03-10T15:34:39.196815Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# X=kendall_tau[\"EMI_AMOUNT\"]\n# X=X[abs(X)>0.4]\n# X","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:40.712093Z","iopub.execute_input":"2024-03-10T15:34:40.712490Z","iopub.status.idle":"2024-03-10T15:34:40.716813Z","shell.execute_reply.started":"2024-03-10T15:34:40.712459Z","shell.execute_reply":"2024-03-10T15:34:40.715794Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# X=spearman_corr[\"EMI_AMOUNT\"]\n# X=X[abs(X)>0.4]\n# X","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:42.862155Z","iopub.execute_input":"2024-03-10T15:34:42.862539Z","iopub.status.idle":"2024-03-10T15:34:42.866575Z","shell.execute_reply.started":"2024-03-10T15:34:42.862512Z","shell.execute_reply":"2024-03-10T15:34:42.865755Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"# # Calculate correlation with 'ACTUAL_ROI'\n# # Calculate correlation with 'ACTUAL_ROI'\n# corr_with_roi = df.corr()['ACTUAL_ROI'].sort_values(ascending=False) = df.corr()['ACTUAL_ROI'].sort_values(ascending=False)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:45.626057Z","iopub.execute_input":"2024-03-10T15:34:45.627002Z","iopub.status.idle":"2024-03-10T15:34:45.631236Z","shell.execute_reply.started":"2024-03-10T15:34:45.626961Z","shell.execute_reply":"2024-03-10T15:34:45.630233Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# spearman_corr = df.corr(method='spearman')\n# kendall_tau = df.corr(method='kendall')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:48.074266Z","iopub.execute_input":"2024-03-10T15:34:48.074890Z","iopub.status.idle":"2024-03-10T15:34:48.078903Z","shell.execute_reply.started":"2024-03-10T15:34:48.074861Z","shell.execute_reply":"2024-03-10T15:34:48.077879Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# spearman_corr[\"ACTUAL_ROI\"]","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:50.573806Z","iopub.execute_input":"2024-03-10T15:34:50.574568Z","iopub.status.idle":"2024-03-10T15:34:50.578469Z","shell.execute_reply.started":"2024-03-10T15:34:50.574539Z","shell.execute_reply":"2024-03-10T15:34:50.577622Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# # corr_with_roi = df.corr()['ACTUAL_ROI'].sort_values(ascending=False) = df.corr()['ACTUAL_ROI'].sort_values(ascending=False)\n# corr_with_roi","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:34:52.742039Z","iopub.execute_input":"2024-03-10T15:34:52.742751Z","iopub.status.idle":"2024-03-10T15:34:52.746532Z","shell.execute_reply.started":"2024-03-10T15:34:52.742712Z","shell.execute_reply":"2024-03-10T15:34:52.745684Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"df1.drop(columns=['ID'], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:35:08.515984Z","iopub.execute_input":"2024-03-10T15:35:08.516367Z","iopub.status.idle":"2024-03-10T15:35:08.530471Z","shell.execute_reply.started":"2024-03-10T15:35:08.516341Z","shell.execute_reply":"2024-03-10T15:35:08.529556Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df = df[df['ACTUAL_ROI'] != 0]\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T14:00:19.363260Z","iopub.execute_input":"2024-03-10T14:00:19.363622Z","iopub.status.idle":"2024-03-10T14:00:19.380224Z","shell.execute_reply.started":"2024-03-10T14:00:19.363591Z","shell.execute_reply":"2024-03-10T14:00:19.379379Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"y=df[\"ACTUAL_ROI\"]\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:35:12.726746Z","iopub.execute_input":"2024-03-10T15:35:12.727455Z","iopub.status.idle":"2024-03-10T15:35:12.731656Z","shell.execute_reply.started":"2024-03-10T15:35:12.727425Z","shell.execute_reply":"2024-03-10T15:35:12.730711Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=y.to_numpy()\nX=df1.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:35:22.054162Z","iopub.execute_input":"2024-03-10T15:35:22.054557Z","iopub.status.idle":"2024-03-10T15:35:22.065201Z","shell.execute_reply.started":"2024-03-10T15:35:22.054529Z","shell.execute_reply":"2024-03-10T15:35:22.064157Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"print(X.shape)\nprint(y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:35:24.336342Z","iopub.execute_input":"2024-03-10T15:35:24.336730Z","iopub.status.idle":"2024-03-10T15:35:24.341998Z","shell.execute_reply.started":"2024-03-10T15:35:24.336700Z","shell.execute_reply":"2024-03-10T15:35:24.341101Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"(69958, 33)\n(69958,)\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:35:37.986659Z","iopub.execute_input":"2024-03-10T15:35:37.987590Z","iopub.status.idle":"2024-03-10T15:35:38.018228Z","shell.execute_reply.started":"2024-03-10T15:35:37.987553Z","shell.execute_reply":"2024-03-10T15:35:38.017258Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nparam_distributions = {\n    'n_estimators': [100, 200, 500, 1000],\n    'max_depth': [6, 10, 15, 20],\n    'min_child_weight': [1, 2, 5, 10],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.5, 1, 1.5, 2],\n    'reg_alpha': [0, 0.1, 0.5, 1, 2],\n    'reg_lambda': [0.1, 1, 5, 10],\n    'scale_pos_weight': [1, 2, 5],\n    'max_delta_step': [0, 1, 2, 5],\n}\n# Initialize the XGBRegressor model with GPU support\n# xgb = XGBRegressor(tree_method='gpu_hist', gpu_id=0)  # Specify tree_method as 'gpu_hist' to use GPU\nxgb = XGBRegressor(tree_method='hist', device='cuda')\n\n# Setup RandomizedSearchCV\nxgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n# Fit the random search model\nxgb_random.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best parameters found: \", xgb_random.best_params_)\n\n# Predict on the validation data using the best parameters\ny_pred = xgb_random.predict(X_val)\n\n# Calculate the RMSE for the validation data\nrmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"Validation RMSE: \", rmse_val)\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T15:35:40.348660Z","iopub.execute_input":"2024-03-10T15:35:40.349384Z","iopub.status.idle":"2024-03-10T16:00:18.559225Z","shell.execute_reply.started":"2024-03-10T15:35:40.349351Z","shell.execute_reply":"2024-03-10T16:00:18.557991Z"},"trusted":true},"execution_count":62,"outputs":[{"name":"stdout","text":"Fitting 3 folds for each of 100 candidates, totalling 300 fits\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:35:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:35:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:35:57] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:36:00] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=6, min_child_weight=5, n_estimators=1000, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=1.0; total time=  18.1s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.2, max_delta_step=1, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=0.7; total time=   1.8s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=   1.8s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=   4.5s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=   4.5s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=1.0; total time=  27.5s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=1000, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1.0; total time=  31.4s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=   9.3s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5, max_depth=15, min_child_weight=2, n_estimators=500, reg_alpha=2, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=  49.8s\n[CV] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.01, max_delta_step=1, max_depth=15, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=  32.0s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.1, max_delta_step=1, max_depth=15, min_child_weight=1, n_estimators=500, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, subsample=0.8; total time=  21.4s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.1, max_delta_step=1, max_depth=15, min_child_weight=1, n_estimators=500, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, subsample=0.8; total time=  20.8s\n[CV] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.7; total time=   3.9s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=   3.1s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=20, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=   7.1s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.05, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  12.0s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.05, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  10.0s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=1000, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  22.9s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time=  27.9s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=1, n_estimators=500, reg_alpha=2, reg_lambda=10, scale_pos_weight=2, subsample=0.6; total time=  15.4s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  20.9s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=5, n_estimators=200, reg_alpha=1, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=  25.9s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=2, n_estimators=1000, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  25.8s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=0.7; total time=  10.5s\n[CV] END colsample_bytree=0.6, gamma=2, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=1, n_estimators=500, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=1.0; total time=  35.1s\n[CV] END colsample_bytree=0.9, gamma=1.5, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=1, n_estimators=1000, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=0.6; total time=  19.8s\n[CV] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=  25.5s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=1, subsample=0.6; total time=   4.1s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=1, subsample=0.6; total time=   3.8s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=20, min_child_weight=10, n_estimators=100, reg_alpha=2, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time=   5.1s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=20, min_child_weight=10, n_estimators=100, reg_alpha=2, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time=   5.1s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=20, min_child_weight=10, n_estimators=100, reg_alpha=2, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time=   3.6s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=   5.5s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=   5.2s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.2, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.5s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=200, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=1, subsample=0.6; total time=   6.0s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=200, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=1, subsample=0.6; total time=   6.6s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=200, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=1, subsample=0.6; total time=   6.8s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=0.9; total time=   3.6s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=0.9; total time=   3.6s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py:752: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:45:01] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.2, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=0.9; total time=  15.1s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.2, max_delta_step=1, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=0.7; total time=   1.8s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.2, max_delta_step=1, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=0.7; total time=   1.7s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=   1.8s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=   1.7s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=   4.5s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=1.0; total time=  27.7s\n[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_delta_step=5, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=   8.8s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=1000, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1.0; total time=  33.7s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5, max_depth=15, min_child_weight=2, n_estimators=500, reg_alpha=2, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=  50.3s\n[CV] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.01, max_delta_step=1, max_depth=15, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=  32.2s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=1000, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  38.3s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=200, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.2s\n[CV] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.7; total time=   4.0s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=   3.0s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=20, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=   6.7s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.2, max_delta_step=1, max_depth=15, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  19.2s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=1000, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  21.9s\n[CV] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=15, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=  33.2s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time=  21.2s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  22.1s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.01, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.6; total time=   6.0s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.01, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.6; total time=   6.6s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=2, n_estimators=1000, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  28.0s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=1.0; total time=  17.2s\n[CV] END colsample_bytree=0.6, gamma=2, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=1, n_estimators=500, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=1.0; total time=  35.3s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=  16.2s\n[CV] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=  25.8s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=100, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=   6.0s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.1, max_delta_step=1, max_depth=20, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=0.9; total time=  15.3s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  10.5s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.2, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.8s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_delta_step=5, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=0.9; total time= 2.9min\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.2, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=0.9; total time=  15.0s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=6, min_child_weight=5, n_estimators=1000, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=1.0; total time=  18.6s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=1.0; total time=  27.0s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=1000, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1.0; total time=  33.4s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  10.3s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5, max_depth=15, min_child_weight=2, n_estimators=500, reg_alpha=2, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=  48.5s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=0.6; total time=   9.0s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.7; total time=   5.9s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.7; total time=   5.8s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=1000, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  39.2s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=200, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   5.2s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=200, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.2s\n[CV] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.7; total time=   4.0s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=   3.1s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=20, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=   6.7s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.2, max_delta_step=1, max_depth=15, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  19.0s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=1000, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  22.4s\n[CV] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=15, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=  32.8s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=1, n_estimators=500, reg_alpha=2, reg_lambda=10, scale_pos_weight=2, subsample=0.6; total time=  15.2s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  20.9s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=5, n_estimators=200, reg_alpha=1, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=  26.0s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=1.0; total time=  23.6s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=0.7; total time=  10.8s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=0.7; total time=  12.7s\n[CV] END colsample_bytree=0.9, gamma=1.5, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=1, n_estimators=1000, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=0.6; total time=  25.1s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=  16.5s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=  21.4s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=100, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=   6.8s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=1, subsample=0.6; total time=   4.1s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.1, max_delta_step=1, max_depth=20, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=0.9; total time=  15.0s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  10.4s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=100, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=   4.9s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_delta_step=5, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=0.9; total time= 2.9min\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.2, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=0.9; total time=  15.1s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=6, min_child_weight=5, n_estimators=1000, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=1.0; total time=  18.6s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=2, reg_lambda=5, scale_pos_weight=5, subsample=0.8; total time=   3.1s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=2, reg_lambda=5, scale_pos_weight=5, subsample=0.8; total time=   3.2s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=2, reg_lambda=5, scale_pos_weight=5, subsample=0.8; total time=   3.1s\n[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_delta_step=5, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=  11.9s\n[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.05, max_delta_step=5, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=   9.6s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.7; total time=  10.5s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.7; total time=  10.3s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=100, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.7; total time=  10.6s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  11.0s\n[CV] END colsample_bytree=0.9, gamma=0.5, learning_rate=0.01, max_delta_step=1, max_depth=15, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=  42.0s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=0.6; total time=   7.5s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=0.6; total time=   9.6s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.7; total time=   5.7s\n[CV] END colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=1000, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  39.8s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.1, max_delta_step=1, max_depth=15, min_child_weight=1, n_estimators=500, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=1, subsample=0.8; total time=  18.6s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.2, max_delta_step=1, max_depth=15, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  19.8s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.05, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  11.0s\n[CV] END colsample_bytree=1.0, gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=15, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=  35.9s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time=  21.9s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=1, n_estimators=500, reg_alpha=2, reg_lambda=10, scale_pos_weight=2, subsample=0.6; total time=  15.8s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=5, n_estimators=200, reg_alpha=1, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=  19.8s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.01, max_delta_step=5, max_depth=6, min_child_weight=2, n_estimators=200, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=0.6; total time=   5.9s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=2, n_estimators=1000, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  28.8s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=0, reg_lambda=10, scale_pos_weight=2, subsample=1.0; total time=  18.5s\n[CV] END colsample_bytree=0.6, gamma=2, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=1, n_estimators=500, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=1, subsample=1.0; total time=  34.7s\n[CV] END colsample_bytree=0.9, gamma=1.5, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=1, n_estimators=1000, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=0.6; total time=  20.0s\n[CV] END colsample_bytree=1.0, gamma=0.1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=  25.5s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=100, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=   6.1s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.1, max_delta_step=1, max_depth=20, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=0.9; total time=  15.2s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.01, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  10.5s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.2, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.9s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_delta_step=5, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=0.9; total time= 2.9min\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:47:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:48:03] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:48:07] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=0.9; total time=   3.8s\n[CV] END colsample_bytree=0.7, gamma=2, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=0.9; total time=  12.9s\n[CV] END colsample_bytree=0.7, gamma=2, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=0.9; total time=  12.8s\n[CV] END colsample_bytree=0.7, gamma=2, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=0.9; total time=  12.7s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=1000, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=  33.6s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=1000, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=  32.5s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=1, n_estimators=1000, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=  30.5s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=1, n_estimators=1000, reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=0.6; total time=  39.9s\n[CV] END colsample_bytree=0.6, gamma=2, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=   2.8s\n[CV] END colsample_bytree=0.6, gamma=2, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=   2.7s\n[CV] END colsample_bytree=0.6, gamma=2, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=   2.4s\n[CV] END colsample_bytree=0.7, gamma=2, learning_rate=0.1, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=   7.6s\n[CV] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=1, subsample=0.9; total time=   6.1s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=1, reg_lambda=5, scale_pos_weight=1, subsample=0.9; total time=   2.2s\n[CV] END colsample_bytree=0.7, gamma=1.5, learning_rate=0.1, max_delta_step=5, max_depth=15, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=0.8; total time=  12.6s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   3.9s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.1s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=5, n_estimators=500, reg_alpha=0, reg_lambda=5, scale_pos_weight=2, subsample=0.7; total time=  10.3s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=1, subsample=0.8; total time=  10.9s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.01, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  16.2s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=5, max_depth=20, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=  17.5s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=5, max_depth=20, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=  15.8s\n[CV] END colsample_bytree=0.7, gamma=1.5, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=   3.5s\n[CV] END colsample_bytree=0.7, gamma=1.5, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=   3.5s\n[CV] END colsample_bytree=0.7, gamma=1.5, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=2, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=   3.2s\n[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=  17.4s\n[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=5, subsample=0.8; total time=  14.8s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.05, max_delta_step=5, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time=  36.7s\n[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=10, n_estimators=200, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  15.9s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=5, scale_pos_weight=1, subsample=0.7; total time=  19.9s\n[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=   7.0s\n[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  10.3s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.2, max_delta_step=5, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=  15.5s\n[CV] END colsample_bytree=0.9, gamma=2, learning_rate=0.05, max_delta_step=2, max_depth=6, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=  17.8s\n[CV] END colsample_bytree=0.9, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=0.9; total time=   7.4s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_delta_step=0, max_depth=20, min_child_weight=2, n_estimators=500, reg_alpha=0, reg_lambda=5, scale_pos_weight=2, subsample=0.9; total time=  26.0s\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:53:15] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.2, max_delta_step=1, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=  13.4s\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.2, max_delta_step=1, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=  13.1s\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.2, max_delta_step=1, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=  11.1s\n[CV] END colsample_bytree=0.7, gamma=2, learning_rate=0.1, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=   7.8s\n[CV] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=1, subsample=0.9; total time=   5.3s\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.05, max_delta_step=1, max_depth=15, min_child_weight=1, n_estimators=100, reg_alpha=2, reg_lambda=1, scale_pos_weight=1, subsample=1.0; total time=   5.4s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  16.2s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.1, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=   4.3s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=5, n_estimators=500, reg_alpha=0, reg_lambda=5, scale_pos_weight=2, subsample=0.7; total time=  10.3s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=1, subsample=0.8; total time=  10.5s\n[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  15.9s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=5, max_depth=20, min_child_weight=1, n_estimators=100, reg_alpha=1, reg_lambda=10, scale_pos_weight=5, subsample=0.6; total time=  18.7s\n[CV] END colsample_bytree=0.9, gamma=2, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=   4.6s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=1, scale_pos_weight=5, subsample=0.6; total time=   8.0s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0, reg_lambda=5, scale_pos_weight=5, subsample=0.7; total time=  19.0s\n[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=  22.6s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.05, max_delta_step=5, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time=  36.8s\n[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=10, n_estimators=200, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  15.6s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=5, scale_pos_weight=1, subsample=0.7; total time=  20.1s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.1, max_delta_step=1, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=0.5, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time=   9.7s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=15, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=5, scale_pos_weight=5, subsample=0.9; total time=  21.1s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.2, max_delta_step=5, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=   9.9s\n[CV] END colsample_bytree=0.9, gamma=2, learning_rate=0.05, max_delta_step=2, max_depth=6, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=  21.0s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_delta_step=0, max_depth=20, min_child_weight=2, n_estimators=500, reg_alpha=0, reg_lambda=5, scale_pos_weight=2, subsample=0.9; total time=  25.7s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_delta_step=5, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=   6.7s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=0, reg_lambda=5, scale_pos_weight=5, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=2, max_depth=20, min_child_weight=5, n_estimators=1000, reg_alpha=2, reg_lambda=1, scale_pos_weight=2, subsample=0.6; total time= 1.6min\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:55:13] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=1, n_estimators=1000, reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=0.6; total time=  42.3s\n[CV] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=1, n_estimators=200, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=1, subsample=0.9; total time=   4.7s\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.05, max_delta_step=1, max_depth=15, min_child_weight=1, n_estimators=100, reg_alpha=2, reg_lambda=1, scale_pos_weight=1, subsample=1.0; total time=   5.7s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=1, reg_lambda=5, scale_pos_weight=1, subsample=0.9; total time=   2.1s\n[CV] END colsample_bytree=0.7, gamma=1.5, learning_rate=0.1, max_delta_step=5, max_depth=15, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=0.8; total time=  12.7s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  14.3s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.1, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=1, subsample=0.8; total time=  10.4s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.01, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  17.3s\n[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  23.7s\n[CV] END colsample_bytree=0.9, gamma=2, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=   4.6s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=1, scale_pos_weight=5, subsample=0.6; total time=   8.1s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0, reg_lambda=5, scale_pos_weight=5, subsample=0.7; total time=  18.9s\n[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=200, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=  22.3s\n[CV] END colsample_bytree=1.0, gamma=1, learning_rate=0.05, max_delta_step=5, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time=  36.6s\n[CV] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=0.6; total time=  21.4s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_delta_step=5, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=5, scale_pos_weight=1, subsample=0.7; total time=  18.7s\n[CV] END colsample_bytree=0.6, gamma=0.5, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=   7.1s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=15, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=5, scale_pos_weight=5, subsample=0.9; total time=  21.5s\n[CV] END colsample_bytree=0.9, gamma=2, learning_rate=0.05, max_delta_step=2, max_depth=6, min_child_weight=2, n_estimators=1000, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=  18.0s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.01, max_delta_step=2, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=0.6; total time=   8.4s\n[CV] END colsample_bytree=0.9, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=0.9; total time=  10.3s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=1, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=   7.3s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=1, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=   6.4s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=1, max_depth=6, min_child_weight=1, n_estimators=200, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=2, subsample=0.9; total time=   4.4s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=   2.1s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_delta_step=5, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=   6.1s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.7; total time=   1.6s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=15, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=   6.1s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=15, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=   7.6s\n[CV] END colsample_bytree=0.8, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=15, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=2, subsample=0.7; total time=   7.9s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.5, reg_lambda=1, scale_pos_weight=1, subsample=0.7; total time=  36.9s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.5, reg_lambda=1, scale_pos_weight=1, subsample=0.7; total time=  33.3s\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=1000, reg_alpha=0.5, reg_lambda=1, scale_pos_weight=1, subsample=0.7; total time=  19.2s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time= 3.1min\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=   2.6s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=0, reg_lambda=5, scale_pos_weight=5, subsample=0.6; total time=   2.3s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.2, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=0, reg_lambda=5, scale_pos_weight=5, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.7; total time=   1.5s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=2, max_depth=20, min_child_weight=5, n_estimators=1000, reg_alpha=2, reg_lambda=1, scale_pos_weight=2, subsample=0.6; total time= 1.6min\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=  16.9s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time= 3.1min\n[CV] END colsample_bytree=0.8, gamma=0.2, learning_rate=0.05, max_delta_step=0, max_depth=10, min_child_weight=1, n_estimators=1000, reg_alpha=1, reg_lambda=1, scale_pos_weight=1, subsample=0.6; total time=  41.0s\n[CV] END colsample_bytree=0.7, gamma=2, learning_rate=0.1, max_delta_step=0, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=   7.7s\n[CV] END colsample_bytree=0.8, gamma=2, learning_rate=0.05, max_delta_step=1, max_depth=15, min_child_weight=1, n_estimators=100, reg_alpha=2, reg_lambda=1, scale_pos_weight=1, subsample=1.0; total time=   5.6s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=6, min_child_weight=2, n_estimators=100, reg_alpha=1, reg_lambda=5, scale_pos_weight=1, subsample=0.9; total time=   2.0s\n[CV] END colsample_bytree=0.7, gamma=1.5, learning_rate=0.1, max_delta_step=5, max_depth=15, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=10, scale_pos_weight=5, subsample=0.8; total time=  12.7s\n[CV] END colsample_bytree=0.8, gamma=1, learning_rate=0.01, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  14.2s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=5, n_estimators=500, reg_alpha=0, reg_lambda=5, scale_pos_weight=2, subsample=0.7; total time=  11.0s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.01, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=1000, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.9; total time=  17.2s\n[CV] END colsample_bytree=0.9, gamma=0.2, learning_rate=0.1, max_delta_step=5, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0.1, reg_lambda=0.1, scale_pos_weight=2, subsample=1.0; total time=  23.7s\n[CV] END colsample_bytree=0.9, gamma=2, learning_rate=0.2, max_delta_step=5, max_depth=6, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=0.8; total time=   4.6s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=1, scale_pos_weight=5, subsample=0.6; total time=   8.1s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0, reg_lambda=5, scale_pos_weight=5, subsample=0.7; total time=  18.9s\n[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=5, subsample=0.8; total time=  15.7s\n[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.2, max_delta_step=2, max_depth=15, min_child_weight=10, n_estimators=200, reg_alpha=0.5, reg_lambda=5, scale_pos_weight=5, subsample=0.8; total time=  13.7s\n[CV] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=0.6; total time=  20.0s\n[CV] END colsample_bytree=0.6, gamma=0.1, learning_rate=0.2, max_delta_step=1, max_depth=10, min_child_weight=5, n_estimators=500, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=5, subsample=0.6; total time=  18.8s\n[CV] END colsample_bytree=0.7, gamma=0.2, learning_rate=0.05, max_delta_step=1, max_depth=20, min_child_weight=10, n_estimators=200, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=1, subsample=0.6; total time=  15.8s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.1, max_delta_step=1, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=0.5, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time=  11.5s\n[CV] END colsample_bytree=1.0, gamma=0.5, learning_rate=0.1, max_delta_step=1, max_depth=6, min_child_weight=10, n_estimators=500, reg_alpha=0.5, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time=   9.7s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=15, min_child_weight=5, n_estimators=200, reg_alpha=2, reg_lambda=5, scale_pos_weight=5, subsample=0.9; total time=  21.1s\n[CV] END colsample_bytree=0.7, gamma=0.5, learning_rate=0.2, max_delta_step=5, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=0, reg_lambda=0.1, scale_pos_weight=5, subsample=0.8; total time=   9.6s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.01, max_delta_step=2, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=0.6; total time=   6.2s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.01, max_delta_step=2, max_depth=10, min_child_weight=2, n_estimators=200, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=0.6; total time=   6.4s\n[CV] END colsample_bytree=0.9, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=15, min_child_weight=5, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=2, subsample=0.9; total time=   8.0s\n[CV] END colsample_bytree=0.7, gamma=0.1, learning_rate=0.2, max_delta_step=0, max_depth=20, min_child_weight=2, n_estimators=500, reg_alpha=0, reg_lambda=5, scale_pos_weight=2, subsample=0.9; total time=  25.7s\n[CV] END colsample_bytree=0.7, gamma=1, learning_rate=0.05, max_delta_step=2, max_depth=6, min_child_weight=1, n_estimators=100, reg_alpha=2, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=   0.8s\n[CV] END colsample_bytree=0.8, gamma=0.5, learning_rate=0.2, max_delta_step=5, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=1, scale_pos_weight=5, subsample=1.0; total time=   6.7s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.2, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.7; total time=   1.4s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.01, max_delta_step=2, max_depth=20, min_child_weight=5, n_estimators=1000, reg_alpha=2, reg_lambda=1, scale_pos_weight=2, subsample=0.6; total time= 1.6min\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=  17.0s\n[CV] END colsample_bytree=0.6, gamma=0, learning_rate=0.01, max_delta_step=2, max_depth=15, min_child_weight=1, n_estimators=1000, reg_alpha=0.1, reg_lambda=1, scale_pos_weight=2, subsample=1.0; total time= 3.1min\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:58:35] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [15:59:58] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:00:02] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"},{"name":"stdout","text":"Best parameters found:  {'subsample': 1.0, 'scale_pos_weight': 2, 'reg_lambda': 10, 'reg_alpha': 0, 'n_estimators': 500, 'min_child_weight': 5, 'max_depth': 10, 'max_delta_step': 0, 'learning_rate': 0.05, 'gamma': 0, 'colsample_bytree': 0.6}\nValidation RMSE:  2.112677842991658\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/xgboost/core.py:160: UserWarning: [16:00:18] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\nPotential solutions:\n- Use a data structure that matches the device ordinal in the booster.\n- Set the device for booster before call to inplace_predict.\n\nThis warning will only be shown once.\n\n  warnings.warn(smsg, UserWarning)\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Predict on the validation data using the best parameters\ny_pred = xgb_random.predict(X_val)\n\n# Calculate the RMSE for the validation data\nrmse_train = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"validation RMSE: \", rmse_train)","metadata":{"execution":{"iopub.status.busy":"2024-03-10T16:00:39.063639Z","iopub.execute_input":"2024-03-10T16:00:39.064726Z","iopub.status.idle":"2024-03-10T16:00:39.124717Z","shell.execute_reply.started":"2024-03-10T16:00:39.064664Z","shell.execute_reply":"2024-03-10T16:00:39.123698Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"validation RMSE:  2.112677842991658\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\ny_pred = xgb_random.predict(X_val)\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    non_zero_mask = y_true != 0\n    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\nmm=y_val==0\nprint(mm.sum())\n# Assuming y_train are the true values and y_pred are the predicted values from your model\nmape_train = mean_absolute_percentage_error(y_val, y_pred)\n\nprint(\"validation MAPE: {:.2f}%\".format(mape_train))\n","metadata":{"execution":{"iopub.status.busy":"2024-03-10T16:00:42.042945Z","iopub.execute_input":"2024-03-10T16:00:42.043689Z","iopub.status.idle":"2024-03-10T16:00:42.079263Z","shell.execute_reply.started":"2024-03-10T16:00:42.043660Z","shell.execute_reply":"2024-03-10T16:00:42.078232Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"0\nvalidation MAPE: 7.25%\n","output_type":"stream"}]},{"cell_type":"code","source":"\n\nfrom joblib import dump\n\n# Assuming xgb_random is your trained RandomizedSearchCV model\n# Save the model to a file\ndump(xgb_random, 'xgb_7.10MAPE.joblib')","metadata":{"execution":{"iopub.status.busy":"2024-03-10T16:00:45.413120Z","iopub.execute_input":"2024-03-10T16:00:45.413972Z","iopub.status.idle":"2024-03-10T16:00:45.494466Z","shell.execute_reply.started":"2024-03-10T16:00:45.413938Z","shell.execute_reply":"2024-03-10T16:00:45.493528Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"['xgb_7.10MAPE.joblib']"},"metadata":{}},{"name":"stdout","text":"[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.01, max_delta_step=2, max_depth=20, min_child_weight=2, n_estimators=1000, reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time= 1.7min\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.1, max_delta_step=2, max_depth=6, min_child_weight=10, n_estimators=1000, reg_alpha=0.5, reg_lambda=10, scale_pos_weight=2, subsample=0.8; total time=  17.5s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=2, reg_lambda=1, scale_pos_weight=5, subsample=0.9; total time=   9.7s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=2, reg_lambda=1, scale_pos_weight=5, subsample=0.9; total time=  10.7s\n[CV] END colsample_bytree=0.9, gamma=0, learning_rate=0.05, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=2, reg_lambda=1, scale_pos_weight=5, subsample=0.9; total time=  10.9s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=1.0; total time=  13.1s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=1.0; total time=  13.0s\n[CV] END colsample_bytree=0.6, gamma=1, learning_rate=0.2, max_delta_step=2, max_depth=10, min_child_weight=2, n_estimators=500, reg_alpha=1, reg_lambda=0.1, scale_pos_weight=1, subsample=1.0; total time=  13.0s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=   3.2s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=   3.6s\n[CV] END colsample_bytree=0.7, gamma=0, learning_rate=0.05, max_delta_step=0, max_depth=6, min_child_weight=10, n_estimators=100, reg_alpha=0.1, reg_lambda=10, scale_pos_weight=5, subsample=1.0; total time=   3.6s\n[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.1, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  10.2s\n[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.1, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  10.1s\n[CV] END colsample_bytree=0.9, gamma=0.1, learning_rate=0.1, max_delta_step=1, max_depth=10, min_child_weight=10, n_estimators=200, reg_alpha=1, reg_lambda=5, scale_pos_weight=2, subsample=1.0; total time=  10.2s\n[CV] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=500, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=1.0; total time=  16.1s\n[CV] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=500, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=1.0; total time=  15.7s\n[CV] END colsample_bytree=1.0, gamma=1.5, learning_rate=0.01, max_delta_step=0, max_depth=6, min_child_weight=5, n_estimators=500, reg_alpha=0.5, reg_lambda=0.1, scale_pos_weight=5, subsample=1.0; total time=  16.0s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=  23.2s\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=   7.1s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.01, max_delta_step=2, max_depth=20, min_child_weight=2, n_estimators=1000, reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time= 1.7min\n[CV] END colsample_bytree=0.6, gamma=1.5, learning_rate=0.1, max_delta_step=2, max_depth=10, min_child_weight=10, n_estimators=500, reg_alpha=0.1, reg_lambda=5, scale_pos_weight=2, subsample=0.6; total time=  17.2s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=5, subsample=0.6; total time=  48.5s\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=5, subsample=0.6; total time=  42.4s\n[CV] END colsample_bytree=1.0, gamma=2, learning_rate=0.01, max_delta_step=2, max_depth=20, min_child_weight=2, n_estimators=1000, reg_alpha=0, reg_lambda=10, scale_pos_weight=1, subsample=0.7; total time= 1.7min\n[CV] END colsample_bytree=0.8, gamma=0.1, learning_rate=0.1, max_delta_step=0, max_depth=20, min_child_weight=10, n_estimators=500, reg_alpha=2, reg_lambda=0.1, scale_pos_weight=5, subsample=0.6; total time=  14.4s\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.isna().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import BaggingClassifier\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom xgboost import XGBClassifier\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import BaggingRegressor\n\ndef create_subsets(X, y, n_subsets):\n    # Use BaggingRegressor for regression tasks\n    bagging = BaggingRegressor(n_estimators=n_subsets, max_samples=1.0/n_subsets, random_state=42)\n    bagging.fit(X, y)\n    \n    # Generate subsets based on the samples chosen by the bagging regressor\n# Adjusted for numpy arrays\n    subsets = [(X[subset_indices], y[subset_indices]) for subset_indices in bagging.estimators_samples_]\n    \n    return subsets\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import Adam\n\ndef train_dnn(subset_x, subset_y, num_features=29, num_neurons=[64, 32], dropout_rate=0.2, learning_rate=0.001):\n    # Define the DNN structure for regression\n    model = Sequential()\n    \n    # Input layer\n    model.add(Dense(num_neurons[0], input_dim=num_features, activation='relu'))\n    model.add(Dropout(dropout_rate))\n    \n    # Hidden layers\n    for neurons in num_neurons[1:]:\n        model.add(Dense(neurons, activation='relu'))\n        model.add(Dropout(dropout_rate))\n    \n    # Output layer for a regression task - no activation function or 'linear' could be used\n    model.add(Dense(1, activation='linear'))\n    \n    # Compile the model for a regression task\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error')\n    \n    # Train the model on the subset\n    model.fit(subset_x, subset_y, epochs=50, batch_size=32, verbose=0)\n    \n    # Return the model including the weights of the last hidden layer\n    # This model can be used as a feature extractor\n    feature_extractor = Sequential()\n    for layer in model.layers[:-1]:  # exclude the output layer\n        feature_extractor.add(layer)\n    \n    return feature_extractor\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_xgboost(features, labels):\n    # Initialize XGBRegressor with GPU support\n    model = XGBRegressor(objective='reg:squarederror', tree_method='gpu_hist')\n    model.fit(features, labels)\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\ndef ensemble_predictions(models, test_features):\n    # Aggregate predictions from each model by averaging\n    predictions = np.array([model.predict(test_features) for model in models])\n    avg_prediction = np.mean(predictions, axis=0)\n    return avg_prediction\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assume X and y are your features and labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nsubsets = create_subsets(X_train, y_train, n_subsets=10)  # number of subsets\nprint(len(subsets[0][0]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"subsets = create_subsets(X_train, y_train, n_subsets=10)  # number of subsets\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Adjusted call to train_dnn within the list comprehension\ndnn_features = [train_dnn(subset_x, subset_y) for subset_x, subset_y in subsets]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list=[]\nfor _,z in subsets:\n    list.append(z)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_array_y = np.concatenate(list, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_array_y.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,10):\n    z=dnn_features[i]\n    dnn_features[i]=z.predict(subsets[i][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,len(dnn_features)):\n    print(dnn_features[i].shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_array = np.concatenate(dnn_features, axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"single_array.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xgboost_models = [train_xgboost(features, y_train) for features in dnn_features]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Get the ensemble prediction\nfinal_prediction = ensemble_predictions(xgboost_models, single_array)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n\n# Assuming y_test are the true values and y_pred are the predicted values from your ensemble model\n# y_pred = ensemble_predictions(xgboost_models, X_test)\n\n# Calculate RMSE\nrmse = np.sqrt(mean_squared_error(single_array, final_prediction))\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\n\n# # Define MAPE function\n# def mape(y_true, y_pred):\n#     y_true, y_pred = np.array(y_true), np.array(y_pred)\n#     non_zero_mask = y_true != 0  # Mask to avoid division by zero\n#     return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n\n# # Calculate MAPE\n# mape_value = mape(y_test, y_pred)\n# print(f\"Mean Absolute Percentage Error (MAPE): {mape_value}%\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nimport numpy as np\n\nparam_distributions = {\n    'n_estimators': [100, 200, 500, 1000],\n    'max_depth': [6, 10, 15, 20],\n    'min_child_weight': [1, 2, 5, 10],\n    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.5, 1, 1.5, 2],\n    'reg_alpha': [0, 0.1, 0.5, 1, 2],\n    'reg_lambda': [0.1, 1, 5, 10],\n    'scale_pos_weight': [1, 2, 5],\n    'max_delta_step': [0, 1, 2, 5],\n}\n# Initialize the XGBRegressor model with GPU support\n# xgb = XGBRegressor(tree_method='gpu_hist', gpu_id=0)  # Specify tree_method as 'gpu_hist' to use GPU\nxgb = XGBRegressor(tree_method='hist', device='cuda')\n\n# Setup RandomizedSearchCV\nxgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n# Fit the random search model\nxgb_random.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best parameters found: \", xgb_random.best_params_)\n\n# Predict on the validation data using the best parameters\ny_pred = xgb_random.predict(X_val)\n\n# Calculate the RMSE for the validation data\nrmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"Validation RMSE: \", rmse_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the XGBRegressor model with GPU support\n# xgb = XGBRegressor(tree_method='gpu_hist', gpu_id=0)  # Specify tree_method as 'gpu_hist' to use GPU\nxgb = XGBRegressor(tree_method='hist', device='cuda')\n\n# Setup RandomizedSearchCV\nxgb_random = RandomizedSearchCV(estimator=xgb, param_distributions=param_distributions, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n\n# Fit the random search model\nxgb_random.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best parameters found: \", xgb_random.best_params_)\n\n# Predict on the validation data using the best parameters\ny_pred = xgb_random.predict(X_val)\n\n# Calculate the RMSE for the validation data\nrmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"Validation RMSE: \", rmse_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Predict on the validation data using the best parameters\ny_pred = xgb_random.predict(X_train)\n\n# Calculate the RMSE for the validation data\nrmse_train = np.sqrt(mean_squared_error(y_train, y_pred))\n\nprint(\"training RMSE: \", rmse_train)\nimport numpy as np\ny_pred = xgb_random.predict(X_val)\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    non_zero_mask = y_true != 0\n    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\nmm=y_val==0\nprint(mm.sum())\n# Assuming y_train are the true values and y_pred are the predicted values from your model\nmape_train = mean_absolute_percentage_error(y_val, y_pred)\n\nprint(\"validation MAPE: {:.2f}%\".format(mape_train))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ny_pred = xgb_random.predict(X_val)\n\ndef mean_absolute_percentage_error(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    non_zero_mask = y_true != 0\n    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\nmm=y_val==0\nprint(mm.sum())\n# Assuming y_train are the true values and y_pred are the predicted values from your model\nmape_train = mean_absolute_percentage_error(y_val, y_pred)\n\nprint(\"validation MAPE: {:.2f}%\".format(mape_train))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.where(\n    df['CURRENT_BALANCE'].notnull() & df['ACTUAL_PAYMT_AMT'].notnull(),\n    df['CURRENT_BALANCE'] / df['ACTUAL_PAYMT_AMT'],\n    np.nan\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=x\narr = arr[~np.isnan(arr)]\n\npercentiles = [np.percentile(arr, p) for p in range(10, 100, 10)]\nprint(percentiles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numbers_greater_than_percentiles = []\n\nfor percentile in range(10, 100, 10):\n    percentile_value = np.nanpercentile(arr, percentile)\n    # Choose a number just greater than the calculated percentile\n    numbers_greater_than_percentiles.append(percentile_value + 0.01)  # Adding a small delta\n\nnumbers_greater_than_percentiles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\ndf['ACTUAL_PAYMT_AMT'].fillna(df['CURRENT_BALANCE'] / random.randint(72, 91), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.where(\n    df['CURRENT_BALANCE'].notnull() & df['COLLATERALVALUE'].notnull(),\n    df['COLLATERALVALUE']/df['CURRENT_BALANCE'],\n    np.nan\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=x\narr = arr[~np.isnan(arr)]\n\npercentiles = [np.percentile(arr, p) for p in range(10, 100, 10)]\nprint(percentiles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numbers_greater_than_percentiles = []\n\nfor percentile in range(10, 100, 10):\n    percentile_value = np.nanpercentile(arr, percentile)\n    # Choose a number just greater than the calculated percentile\n    numbers_greater_than_percentiles.append(percentile_value + 0.01)  # Adding a small delta\n\nnumbers_greater_than_percentiles","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['COLLATERALVALUE'].fillna(df['CURRENT_BALANCE'] * random.uniform(1.8, 2.5), inplace=True)\ndf['REPAYMENT_TENURE'].fillna(df['HIGH_CREDIT_OR_SANCTIONED_AMOUNT'] / random.randint(6600, 12000), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = np.where(\n    df['REPAYMENT_TENURE'].notnull() & df['HIGH_CREDIT_OR_SANCTIONED_AMOUNT'].notnull(),\n    df['HIGH_CREDIT_OR_SANCTIONED_AMOUNT']/df['REPAYMENT_TENURE'],\n    np.nan\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"arr=x\narr = arr[~np.isnan(arr)]\n\npercentiles = [np.percentile(arr, p) for p in range(10, 100, 10)]\nprint(percentiles)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['REPAYMENT_TENURE'].fillna(df['HIGH_CREDIT_OR_SANCTIONED_AMOUNT'] / random.randint(6600, 12000), inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mode_values = df['DATE_OF_LAST_PAYMENT'].mode().head()\n\n# Replace null values with one of the top 5 mode values randomly\nnull_indices = df[df['DATE_OF_LAST_PAYMENT'].isnull()].index\nfor index in null_indices:\n    df.at[index, 'DATE_OF_LAST_PAYMENT'] = np.random.choice(mode_values)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"null_counts = df.isnull().sum()\n# Printing the count of null values for each column\nprint(null_counts)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_payment_history(entry):\n    # Replacement rules\n    replacements = {'STD': '000', 'SMA': '060', 'SUB': '090', 'DBT': '090', 'LSS': '090', 'XXX': 'XXX'}\n    \n    # Replace according to rules\n    for key, value in replacements.items():\n        entry = entry.replace(key, value)\n    \n    # Initialize counts and total for average calculation\n    count_xxx = entry.count('XXX')\n    count_060 = entry.count('060')\n    count_090 = entry.count('090')\n    nums = []  \n    i = 0\n\n    while i < len(entry):\n        group = entry[i:i+3]  \n\n        if 'X' in group:\n            number_before_x = ''.join([char for char in group if char.isdigit()])\n            if number_before_x:  \n                nums.append(int(number_before_x))\n\n            i += group.find('X')  \n            while i < len(entry) and entry[i] == 'X':\n                i += 1  # Skip the 'X's\n        else:\n            try:\n                if group!=\"060\" and group!=\"090\":\n                    nums.append(int(group))\n            except ValueError:\n                # This catches cases where conversion to integer might fail\n                pass\n            i += 3  # Move to the next group\n    \n    avg_nums = sum(nums) / len(nums) if nums else 0\n\n    return avg_nums, count_xxx, count_060, count_090\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the DataFrame\nresults = df['PAYMENT_HISTORY_1'].apply(process_payment_history)\n\n# Split the results into separate columns\ndf['avg_nums'], df['count_xxx'], df['count_060'], df['count_090'] = zip(*results)\n# Apply the function to the DataFrame\nresults = df['PAYMENT_HISTORY_2'].apply(process_payment_history)\n\n# Split the results into separate columns\ndf['avg_nums2'], df['count_xxx2'], df['count_0602'], df['count_0902'] = zip(*results)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply the function to the DataFrame\nresults = df['PAYMENT_HISTORY_2'].apply(process_payment_history)\n\n# Split the results into separate columns\ndf['avg_nums2'], df['count_xxx2'], df['count_0602'], df['count_0902'] = zip(*results)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['PAYMENT_HISTORY_2', 'PAYMENT_HISTORY_1'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df['DATE_OF_BIRTH'] = pd.to_datetime(df['DATE_OF_BIRTH'], format='%d-%m-%Y')\ndf['DATE_OPENED'] = pd.to_datetime(df['DATE_OPENED'], format='%d-%m-%Y')\n\n# Calculate the age at the time of account opening\ndf['age_at_opening'] = df['DATE_OPENED'].dt.year - df['DATE_OF_BIRTH'].dt.year\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['DATE_OPENED', 'DATE_OF_BIRTH'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['DATE_OF_LAST_PAYMENT'], axis=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bool_columns = df.select_dtypes(include=bool).columns\ndf[bool_columns] = df[bool_columns].astype(int)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[:5]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"2.5\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.metrics import RootMeanSquaredError\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=newdf[\"ACTUAL_ROI\"]\nnewdf=newdf.drop(\"ACTUAL_ROI\",axis=1)\ny=y.to_numpy()\nX=newdf.to_numpy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y=y.to_numpy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"newdf=newdf.to_numpy()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X=newdf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming a regression task, change the output layer and loss function accordingly\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(40,)),  # Assuming 40 input features\n    Dense(64, activation='relu'),\n    Dense(1, activation='linear')  # Output layer for regression\n])\n\n# Compile the model with RMSE as a metric\nmodel.compile(optimizer='adam',\n              loss='mean_squared_error',  # Use MSE loss for regression tasks\n              metrics=[RootMeanSquaredError(name='rmse')])  # Add RMSE as a metric\n\n# Summary of the model\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history = model.fit(X_train, y_train,          # X_train is your input features, y_train are the labels\n                    batch_size=32,             # Batch size (optional, default is 32)\n                    epochs=10,                 # Number of epochs\n                    validation_data=(X_val, y_val),  # Validation data (optional)\n                    verbose=1)    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\n\n# Assuming X, y are your features and target variable, respectively\n\n# Split data into training and validation sets\n# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the RandomForestRegressor model\nmodel = RandomForestRegressor()\n\n# Fit the model on the training data\nmodel.fit(X_train, y_train)\n\n# Predict on the validation data\ny_pred = model.predict(X_val)\n\n# Calculate the RMSE for the validation data\nrmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"Validation RMSE: \", rmse_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split, RandomizedSearchCV\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Hyperparameter grid\nparam_distributions = {\n    'n_estimators': [100, 200, 500],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'max_features': ['auto', 'sqrt'],\n    'bootstrap': [True, False]\n}\n\n# Initialize the RandomForestRegressor model\nrf = RandomForestRegressor()\n\n# Setup RandomizedSearchCV\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = param_distributions, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)\n\n# Best parameters\nprint(\"Best parameters found: \", rf_random.best_params_)\n\n# Predict on the validation data using the best parameters\ny_pred = rf_random.predict(X_val)\n\n# Calculate the RMSE for the validation data\nrmse_val = np.sqrt(mean_squared_error(y_val, y_pred))\n\nprint(\"Validation RMSE: \", rmse_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from joblib import dump, load\n\n# Save the model to a file\ndump(model, 'random_forest_regressor.joblib')\n\n# Now, you can load it back at any time with\nloaded_model = load('random_forest_regressor.joblib')\n\n# And then use it to make predictions\n# y_pred_loaded = loaded_model.predict(X_val)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}